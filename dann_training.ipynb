{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.applications import EfficientNetB0\n",
    "from keras import layers\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3587 files belonging to 6 classes.\n",
      "Found 2527 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "RESOLUTION = 224\n",
    "\n",
    "realwaste = keras.utils.image_dataset_from_directory(\n",
    "    \"./data/realwaste\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(RESOLUTION, RESOLUTION),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "trashnet = keras.utils.image_dataset_from_directory(\n",
    "    \"./data/trashnet\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(RESOLUTION, RESOLUTION),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset, train_pct):\n",
    "        size = len(list(dataset.as_numpy_iterator()))\n",
    "        train = dataset.take(int(train_pct * size))\n",
    "        validation = dataset.skip(int(train_pct * size))\n",
    "        return train, validation\n",
    "\n",
    "training_dataset, validation_dataset = split(realwaste, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_layers = [\n",
    "    # Lighting variations\n",
    "    layers.RandomBrightness(factor=(-0.2, 0.2)),\n",
    "    # Blurring\n",
    "    layers.GaussianNoise(stddev=0.2),\n",
    "    # Distortions\n",
    "    layers.RandomRotation(factor=0.1, fill_mode='nearest'),\n",
    "    layers.RandomFlip(mode='horizontal'),\n",
    "    layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n",
    "    layers.RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode='nearest'),\n",
    "    # Color variations\n",
    "    layers.RandomContrast(factor=(0.8, 1.2)),\n",
    "]\n",
    "\n",
    "def augment(image):\n",
    "    for layer in augmentation_layers:\n",
    "        image = layer(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "\n",
    "def preprocess_augment(image, label):\n",
    "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
    "    image = augment(image)\n",
    "    return image, label\n",
    "\n",
    "def resize(image, label):\n",
    "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
    "    return image, label\n",
    "\n",
    "# Preprocess training\n",
    "training_dataset = training_dataset.map(preprocess_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "training_dataset = training_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "validation_dataset = validation_dataset.map(resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "trashnet = trashnet.map(resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "trashnet = trashnet.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor(input_shape):\n",
    "    base_model = EfficientNetB0(include_top=False, weights=None, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return keras.Model(base_model.input, x)\n",
    "\n",
    "# Label Classifier\n",
    "def build_label_classifier(feature_extractor):\n",
    "    x = feature_extractor.output\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    return keras.Model(feature_extractor.input, x)\n",
    "\n",
    "# Domain Classifier\n",
    "def build_domain_classifier(feature_extractor):\n",
    "    x = feature_extractor.output\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return keras.Model(feature_extractor.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dann_model(feature_extractor, label_classifier, domain_classifier):\n",
    "    input_image = layers.Input(shape=(RESOLUTION, RESOLUTION, 3))\n",
    "    \n",
    "    # Feature extraction\n",
    "    features = feature_extractor(input_image)\n",
    "    \n",
    "    # Label prediction\n",
    "    label_pred = label_classifier(input_image)\n",
    "    \n",
    "    # Domain prediction\n",
    "    domain_pred = domain_classifier(input_image)\n",
    "    \n",
    "    return keras.Model(input_image, [label_pred, domain_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "domain_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Define the metrics\n",
    "label_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "domain_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(source_images, source_labels, target_images):\n",
    "    with tf.GradientTape() as tape:\n",
    "        source_label_preds, source_domain_preds = dann_model(source_images)\n",
    "        source_label_loss = label_loss_fn(source_labels, source_label_preds)\n",
    "        source_domain_loss = domain_loss_fn(tf.zeros_like(source_domain_preds), source_domain_preds)\n",
    "        \n",
    "        _, target_domain_preds = dann_model(target_images)\n",
    "        target_domain_loss = domain_loss_fn(tf.ones_like(target_domain_preds), target_domain_preds)\n",
    "        \n",
    "        total_loss = source_label_loss + source_domain_loss + target_domain_loss\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, dann_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, dann_model.trainable_variables))\n",
    "    \n",
    "    label_accuracy.update_state(source_labels, source_label_preds)\n",
    "    domain_accuracy.update_state(tf.zeros_like(source_domain_preds), source_domain_preds)\n",
    "    domain_accuracy.update_state(tf.ones_like(target_domain_preds), target_domain_preds)\n",
    "\n",
    "# Training loop\n",
    "def train_dann(training_dataset, trashnet, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        label_accuracy.reset_state()\n",
    "        domain_accuracy.reset_state()\n",
    "        \n",
    "        for source_images, source_labels in training_dataset:\n",
    "            target_images, _ = next(iter(trashnet))\n",
    "            train_step(source_images, source_labels, target_images)\n",
    "        \n",
    "        print(f\"Label Accuracy: {label_accuracy.result().numpy()}, Domain Accuracy: {domain_accuracy.result().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Label Accuracy: 0.25, Domain Accuracy: 0.5001736283302307\n",
      "Epoch 2/10\n",
      "Label Accuracy: 0.24166665971279144, Domain Accuracy: 0.5055555701255798\n",
      "Epoch 3/10\n",
      "Label Accuracy: 0.24409721791744232, Domain Accuracy: 0.510937511920929\n",
      "Epoch 4/10\n",
      "Label Accuracy: 0.24930556118488312, Domain Accuracy: 0.6421874761581421\n",
      "Epoch 5/10\n",
      "Label Accuracy: 0.2777777910232544, Domain Accuracy: 0.770312488079071\n",
      "Epoch 6/10\n",
      "Label Accuracy: 0.3027777671813965, Domain Accuracy: 0.7741319537162781\n",
      "Epoch 7/10\n",
      "Label Accuracy: 0.2993055582046509, Domain Accuracy: 0.7769097089767456\n",
      "Epoch 8/10\n",
      "Label Accuracy: 0.3402777910232544, Domain Accuracy: 0.8092013597488403\n",
      "Epoch 9/10\n",
      "Label Accuracy: 0.3409722149372101, Domain Accuracy: 0.8192708492279053\n",
      "Epoch 10/10\n",
      "Label Accuracy: 0.3583333194255829, Domain Accuracy: 0.8496527671813965\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = build_feature_extractor((RESOLUTION, RESOLUTION, 3))\n",
    "label_classifier = build_label_classifier(feature_extractor)\n",
    "domain_classifier = build_domain_classifier(feature_extractor)\n",
    "dann_model = build_dann_model(feature_extractor, label_classifier, domain_classifier)\n",
    "\n",
    "dann_model.compile(optimizer=optimizer)\n",
    "\n",
    "train_dann(training_dataset, trashnet, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
